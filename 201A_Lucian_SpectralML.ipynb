{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlKxpxlUyD_-"
      },
      "source": [
        "This notebook makes use of the analysis-resynthesis approach to predict the sound of new audio based on an existing corpus.\n",
        "\n",
        "The example material ~1 hour of my released music.\n",
        "\n",
        "Goals for this notebook were:\n",
        "\n",
        "1. Generate novel harmonic and textural material for composition from my existing material.\n",
        "2. Examine if trends in the spectral analysis of the audio reflect conscious compositional intentions.\n",
        "\n",
        "  (this was not as explored due to time constraints with length of computing time)\n",
        "3. Provide an easy open source framework for others to produce new sounds from existing bodies of work.\n",
        "\n",
        "  (potential future goal - this project is ineffecient in its current cloud state)\n",
        "\n",
        "  Source Material:\n",
        "  https://open.spotify.com/artist/5etGw4ubkJ9urWZ9OYBm7U?si=v8I70nj8Tjuglc9Y7QXktg\n",
        "  Example output from notebook:\n",
        "  https://drive.google.com/drive/folders/1jIcM5dzG6EVX038Tmd7WyRiE9l0or5ZY?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGMqR_2bhM14",
        "outputId": "9a6c24a6-7c4e-42b2-bd88-7738aed8234f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "import os\n",
        "import math\n",
        "import sklearn\n",
        "from sklearn.cluster import KMeans\n",
        "#from sklearn.decomposition import PC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVR #svr for non-linear, regression tasks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import norm\n",
        "from scipy.signal import convolve\n",
        "from scipy.io.wavfile import write\n",
        "import scipy\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use corpusPath to set audio folder location. Must be folder of wav files. Non-wav files will be skipped during read in. Set the desired length of predicted clips below"
      ],
      "metadata": {
        "id": "CutjVSMrmfhI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssK7VXHkg3_U"
      },
      "outputs": [],
      "source": [
        "corpusPath = '/content/drive/Shareddrives/content/201-Final-Corpus-Lucian'\n",
        "\n",
        "corpusSize = len(os.listdir(corpusPath))\n",
        "corpusList = (os.listdir(corpusPath))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desiredClipLength = 5 #in seconds"
      ],
      "metadata": {
        "id": "W8ZwXUFQ3YVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHPBm8l-rbiG"
      },
      "source": [
        "THIS SECTION READS IN AUDIO AND FORMATS THE DATA FOR MANIPULATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEEQKqrUe0h3"
      },
      "outputs": [],
      "source": [
        "def updateSampRate(track):\n",
        "  sampRate = librosa.get_samplerate((corpusPath + '/' + corpusList[track]))\n",
        "  return sampRate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftJQCOUE07FR"
      },
      "outputs": [],
      "source": [
        "SAMPLERATE = updateSampRate(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyhPMlZ813A3"
      },
      "outputs": [],
      "source": [
        "def setSliceLength(seconds):\n",
        "  return SAMPLERATE * seconds #samples per slice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAqXiB_haLWs",
        "outputId": "c95f4112-9ad8-40ad-9c50-7ecbeeeccbcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "441000"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sliceLength = setSliceLength(10) #10 seconds\n",
        "sliceLength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8xZbKrAiE2T"
      },
      "outputs": [],
      "source": [
        "def storeTracks(corpusList, corpusPath):\n",
        "  rawTracks = []\n",
        "  for i in range(len(corpusList)):\n",
        "    loadName = corpusPath + \"/\" + corpusList[i]\n",
        "    if '.wav' in loadName:\n",
        "      data, SAMPLERATE = librosa.load(loadName)\n",
        "      for i in range(len(data)):\n",
        "        rawTracks.append(data[i])\n",
        "\n",
        "  samplesArray = np.asarray(rawTracks)\n",
        "  numClips = math.floor(len(samplesArray) / sliceLength)\n",
        "  #information is shaved off here for reshaping, but could instead be padded to keep all info\n",
        "  flatClipsArray = np.reshape((samplesArray[:(numClips*sliceLength)]), (numClips, sliceLength))\n",
        "  print (flatClipsArray.shape, type(flatClipsArray))\n",
        "  return flatClipsArray\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAATzIGYmkfx",
        "outputId": "e1c87016-db35-455f-ff4c-d18337f4640d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(194, 441000) <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "clipsArray = storeTracks(corpusList, corpusPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9imdUxDqVN6"
      },
      "source": [
        "THIS SECTION IS MAKING A FEATURE VECTOR OF ALL DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIbB8L1oBAIY"
      },
      "source": [
        "Function below makes a feature vector for each clip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUTfPVLmqauT"
      },
      "outputs": [],
      "source": [
        "def makeClipFeatureVector(inputClip):\n",
        "\n",
        "  vector = []\n",
        "\n",
        "  vector.append(librosa.feature.spectral_centroid(y=inputClip, sr=SAMPLERATE))\n",
        "  vector.append(librosa.feature.spectral_flatness(y=inputClip))\n",
        "  vector.append(librosa.feature.spectral_rolloff(y=inputClip))\n",
        "  vector.append(librosa.feature.zero_crossing_rate(inputClip))\n",
        "  vector.append(librosa.feature.rms(y=inputClip))\n",
        "  mfcc = librosa.feature.mfcc(y=inputClip)\n",
        "  for i in range(mfcc.shape[0]):\n",
        "    element = mfcc[i]\n",
        "    element = np.reshape((element), (1, element.shape[0]))\n",
        "    vector.append(element)\n",
        "\n",
        "  newVector =  np.asarray(vector)\n",
        "  return np.reshape((newVector), (newVector.shape[0], newVector.shape[2]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkA7Ui8NAvao"
      },
      "source": [
        "Function below uses makeClipFeatureVector function to create feature vectors for every clip, store them in a list, and than converts that to an array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O27484IruBVY"
      },
      "outputs": [],
      "source": [
        "def makeClipListVectors(allClips):\n",
        "  allFeatureVectors = []\n",
        "  for i in range(len(allClips)): #change back to range totalClip\n",
        "    clip = clipsArray[i]\n",
        "    featureVector = makeClipFeatureVector(clip)\n",
        "    #vectorPairs = [i, featureVector]\n",
        "    #print(vectorPairs)\n",
        "    allFeatureVectors.append(featureVector)\n",
        "  return np.asarray(allFeatureVectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSy7FAzajC3P"
      },
      "outputs": [],
      "source": [
        "allClipsFeatureVectors = makeClipListVectors(clipsArray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib8cjaKRjPY5",
        "outputId": "20f5b379-628b-4cf1-a776-ba48e7fb4cc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(194, 25, 862)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "allClipsFeatureVectors.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQuFTW5bBINn"
      },
      "source": [
        "Creates clusters of all clips, stores in a list. Clustering is based on above feature vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xy6vSZlZu2Le"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8myS9ftWSiB"
      },
      "outputs": [],
      "source": [
        "#reshape all feature vectors to be 2d for clustering\n",
        "reshapedX = np.reshape((allClipsFeatureVectors), (allClipsFeatureVectors.shape[0],allClipsFeatureVectors.shape[1]*allClipsFeatureVectors.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8H0vs5vBFlg"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=30, random_state=42) #n clusters = desired Clips\n",
        "#need to work on slihouetting for idea number of clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtLEcDa9IwmR"
      },
      "outputs": [],
      "source": [
        "MainCluster = kmeans.fit_predict(reshapedX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0pcZzTxEerW"
      },
      "outputs": [],
      "source": [
        "def makeClusterList(inputCluster):\n",
        "  clusterList = []\n",
        "  for i in range((kmeans.n_clusters)):\n",
        "    clusterIndex = np.where(inputCluster == i)\n",
        "    clusterList.append(clusterIndex)\n",
        "  return clusterList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EN5YK9nEylH"
      },
      "outputs": [],
      "source": [
        "clusterList = makeClusterList(MainCluster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efyvfqKKbVRd"
      },
      "source": [
        "Playback audio of a specific cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH8sSDQIbYld"
      },
      "outputs": [],
      "source": [
        "def arrayFromCluster(cluster):\n",
        "  storeClips = []\n",
        "  for i in range(len(cluster)):\n",
        "    clipNum = cluster[i]\n",
        "    storeClips.append(clipsArray[clipNum])\n",
        "  audioStack = np.asarray(storeClips)\n",
        "  reshapedStack = np.reshape(audioStack, (audioStack.shape[1] * audioStack.shape[2]))\n",
        "  print(reshapedStack.shape)\n",
        "  return reshapedStack\n",
        "\n",
        "def playClusterClips(array):\n",
        "  return Audio(data=array, rate=SAMPLERATE/2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OUzHmrCwNw8"
      },
      "source": [
        "ANALYSIS / RESYNTHESIS\n",
        "\n",
        "First, we take in a cluster of clips and read their FFTs. We create a new array with all the stored fft information, organize by clips (this is for training purposes, to avoid 1 continuous time series.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gDWaGZC9PNd"
      },
      "outputs": [],
      "source": [
        "def storeClusterFFT(cluster):\n",
        "  storeClips = []\n",
        "  for i in range(len(cluster)):\n",
        "    clipNum = cluster[i]\n",
        "    storeClips.append(clipsArray[clipNum])\n",
        "  audioStack = np.asarray(storeClips)\n",
        "  reshapedStack = np.reshape(audioStack, (audioStack.shape[1], audioStack.shape[2]))\n",
        "  StoreFFTs = []\n",
        "  for i in range((audioStack.shape[1])):\n",
        "    clip = reshapedStack[i]\n",
        "    clipFFT = np.abs(librosa.stft(clip))\n",
        "    StoreFFTs.append(clipFFT) #(clipFFT.T) #instead of time steps at each freq bin, Transposes to be freq bins at each timestep\n",
        "  arrayFFT = np.asarray(StoreFFTs)\n",
        "  return arrayFFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_4X_vUnykz3"
      },
      "outputs": [],
      "source": [
        "newClusterFFTTest = storeClusterFFT(clusterList[5])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newClusterFFTTest6 = storeClusterFFT(clusterList[8])"
      ],
      "metadata": {
        "id": "Ole4mKFtUhYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we train a set of models for the cluster. This takes several minutes, as we have the setting tunes precisely to capture interesting spectral evolution, rather than a more vague spectral profile.\n",
        "\n",
        "We are using Support Vector Regression to analyze non-linear relationships. There is a model for each bin, trained on a time series of magnitude for each clip, resulting in bin amount of models. I took a lookback / windowing approach to account for time. the alternative was a sort of nueral net approach that i found ineffective."
      ],
      "metadata": {
        "id": "xgLb0Ppi0xSx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lq2gO2YR1_5"
      },
      "outputs": [],
      "source": [
        "def trainBinModels(FFTArray, lookback=50, C=20.0):\n",
        "  #lookback=50 covers ~1.15s of history, which is good for smooth spectral evolution\n",
        "  #A lookback=100 covers ~2.3s, making it better for longer-term spectral shaping\n",
        "\n",
        "  nClips, nBins, nTimeSteps = FFTArray.shape\n",
        "  modelsList = [SVR(kernel='rbf', C=C, epsilon=0.001) for _ in range(nBins)] #creates a new model for each bin, we will train these bin models across clips\n",
        "  #epsilon deals with microvariation. the lower the more precise\n",
        "  #C controls how flexible SVR is when fitting the data\n",
        "  for bin in tqdm(range(nBins)):  # Train one model per frequency bin\n",
        "        X_train, y_train = [], []\n",
        "        for i in range(nClips): #fetches training data (time series of magnitudes) for each clip at specific bin.\n",
        "            currentBin = FFTArray[i, bin, :]\n",
        "            for j in range(len(currentBin) - lookback):\n",
        "                X_train.append(currentBin[j:j+lookback])\n",
        "                y_train.append(currentBin[j+lookback])\n",
        "        #x training for this bin\n",
        "        X_train = np.array(X_train)\n",
        "        #y training for this bin\n",
        "        y_train = np.array(y_train)\n",
        "\n",
        "        #fit training for model at the specific bin\n",
        "        modelsList[bin].fit(X_train, y_train)\n",
        "  return modelsList\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use this to calculate how long our predicted clips will be (seconds -> timesteps)"
      ],
      "metadata": {
        "id": "4Ny4Njau1wNS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zysLNBCkl7M7"
      },
      "outputs": [],
      "source": [
        "def calcTimeSteps(goal):\n",
        "  #goal in seconds\n",
        "  return int((goal * (SAMPLERATE/2)) / 512) #hoplength\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrdYlvT0mVbq",
        "outputId": "1ada080c-6245-47f9-c466-1aa84fd10463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215\n"
          ]
        }
      ],
      "source": [
        "nFutureTimeSteps = calcTimeSteps(desiredClipLength) #currently set to 5 seconds\n",
        "print(nFutureTimeSteps)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainedBinModels = trainBinModels(newClusterFFTTest6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "S7BU8MQwEEV9",
        "outputId": "cb9d47ae-3cdc-4d60-f01a-13bdb2417f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 31/1025 [08:22<4:28:17, 16.19s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-dd88b6ad60e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainedBinModels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainBinModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewClusterFFTTest6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-007f665d02b9>\u001b[0m in \u001b[0;36mtrainBinModels\u001b[0;34m(FFTArray, lookback, C)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#fit training for model at the specific bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmodelsList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodelsList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_status_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibsvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_libsvm.pyx\u001b[0m in \u001b[0;36msklearn.svm._libsvm.fit\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes our initial cluster array, the trained models, and our desired clip length and produces arrays of predicted ffts. These are then used for resynthesis."
      ],
      "metadata": {
        "id": "fPL9PwWF17P1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ-lvengbUmI"
      },
      "outputs": [],
      "source": [
        "def predictFromBinModels(FFTArray, models, nFutureSteps=50, lookback=50):\n",
        "  nClips, nBins, nTimeSteps = FFTArray.shape\n",
        "  predictionsArray = np.zeros((nClips, nBins, nTimeSteps + nFutureSteps))\n",
        "\n",
        "  #loop through clips\n",
        "  for i in tqdm(range(nClips), position=0,leave=True):\n",
        "      data = FFTArray[i]\n",
        "\n",
        "      for bin in range(nBins):\n",
        "        currentBin = data[bin, :] #time series for current bin\n",
        "        predictedTimeSeries = list(currentBin[:])  # Start with real data\n",
        "\n",
        "        currentModel = trainedBinModels[bin]\n",
        "\n",
        "        #initializing sequency for predicting time series\n",
        "        lastSeq = np.array(currentBin[-lookback:]).reshape(1, -1)\n",
        "\n",
        "        #predict future values\n",
        "        for n in range(nFutureSteps):\n",
        "          predictedNextValue = currentModel.predict(lastSeq)[0]\n",
        "          predictedTimeSeries.append(predictedNextValue)\n",
        "          lastSeq = np.roll(lastSeq, -1) #used for rercursive prediction\n",
        "          lastSeq[0, -1] = predictedNextValue\n",
        "\n",
        "        #after loop of predicting values:\n",
        "\n",
        "        predictionsArray[i, bin, nTimeSteps:] = predictedTimeSeries[-nFutureSteps:]\n",
        "  return predictionsArray[:, :, -nFutureSteps:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jGAZN3ohjVj"
      },
      "outputs": [],
      "source": [
        "testPrediction = predictFromBinModels(newClusterFFTTest6, trainedBinModels, nFutureTimeSteps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section takes the n predicted FFT arrays (based on number of clips in the cluster), and convolves them using a gassian convolution kernel. This is done rather than using another machine learning model, when this process is already very slow. For the first few examples, I used this convolution approach.\n",
        "After the first few clusters, I realized that I prefered to have seperate clips, as they already contained a significant amount of spectral information - so I stopped using convolution."
      ],
      "metadata": {
        "id": "eO2kDWax_Y5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a gaussian kernel for convolution\n",
        "\n",
        "def gaussian_kernel(size, std):\n",
        "    x = np.linspace(-size//2, size//2, size)\n",
        "    kernel = norm.pdf(x, scale=std)\n",
        "    return kernel / kernel.sum()  # Normalize\n",
        "\n",
        "# 3point Gaussian kernel\n",
        "gKernel = gaussian_kernel(3, 0.5).reshape(3, 1, 1) # need to update this so it is n clips"
      ],
      "metadata": {
        "id": "mBaqiGjzAwzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convolveClipsPrediction(prediction, kernel):\n",
        "  #currently hard coded to 3 clips, will need to update\n",
        "  kernel = np.ones((6, 1, 1)) / 6\n",
        "  predList = []\n",
        "  for i in range(prediction.shape[0]):\n",
        "    predList.append(prediction[i])\n",
        "\n",
        "  finalPrediction = convolve(prediction, kernel, mode=\"valid\")[0]\n",
        "  return finalPrediction"
      ],
      "metadata": {
        "id": "Rw95e-d89rRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convolved = convolveClipsPrediction(testPrediction, gKernel)"
      ],
      "metadata": {
        "id": "fTNYl9kO-GpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function optionally makes predictions based on the entire corpus. I am not using it but it would be useful on a smaller corpus. On my hour long corpus, this would take hours if not days."
      ],
      "metadata": {
        "id": "A4S8ZuyRuuew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mainPredictionList = []\n",
        "def createPredictionsList(clusterList, nClips):\n",
        "  for clust in range(clusterList.shape[0]):\n",
        "    clusterFFT = storeClusterFFT(clusterList[clust])\n",
        "    currentModel = trainBinModels(clusterFFT)\n",
        "    prediction = predictFromBinModels(clusterFFT, currentModel, nFutureTimeSteps)\n",
        "    for clip in range(prediction.shape[0]):\n",
        "      mainPredictionList.append(prediction[clip])\n"
      ],
      "metadata": {
        "id": "Lo-hYWW0kEZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below takes a predicted FFT array and resynthesizes it via ISTFT. It also writes the audio to a new wav file."
      ],
      "metadata": {
        "id": "dm8_A_J7vAhI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AidjEfDAsBjc"
      },
      "outputs": [],
      "source": [
        "#used to index into single clip of a predicted cluster\n",
        "def resynthSinglePrediction(prediction):\n",
        "  data =  librosa.istft(prediction)\n",
        "  write('new.wav', int(SAMPLERATE/2), data)\n",
        "  return Audio(data=data, rate=SAMPLERATE/2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#used to read the whole predicted cluster as a longer audio clip\n",
        "def resynthPredictionLong(prediction):\n",
        "  tempList = []\n",
        "  for i in range(prediction.shape[0]):\n",
        "    for j in range(prediction.shape[1])\n",
        "    tempList.append(tempList[i][j])\n",
        "  predict = np.asarray(tempList)\n",
        "  data =  librosa.istft(predict)\n",
        "  write('new.wav', int(SAMPLERATE/2), data)\n",
        "  return Audio(data=data, rate=SAMPLERATE/2)"
      ],
      "metadata": {
        "id": "j3hBQXrMpZnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resynthPredictionLong(testPrediction)"
      ],
      "metadata": {
        "id": "6Zq4AU97w-O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function returns audio of the entire prediction list if it is needed."
      ],
      "metadata": {
        "id": "_143RQEcwuE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def resynthAllPredictions(predictionList):\n",
        "  tempList = []\n",
        "  predictions = np.asarray(predictionList)\n",
        "  for l in range(len(predictionList)):\n",
        "    store = resynthSinglePrediction(predictions[l])\n",
        "    tempList.append(store)\n"
      ],
      "metadata": {
        "id": "EfeZQErdj7-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}